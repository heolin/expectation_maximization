{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM Model1\n",
    "### Podstawowe równanie\n",
    "Główne równanie machine translation\n",
    "\n",
    "$$p(\\mathrm{f},\\mathrm{a}|\\mathrm{e}) = \\frac{\\epsilon}{(l+1)^m} \\prod_{j=1}^m t(f_j|e_{a(j)})$$\n",
    "\n",
    "Gdzie:\n",
    "- zdanie $\\mathrm{f}$ to zdanie zagraniczne $\\mathrm{f}=f_1...f_m$,\n",
    "- zdanie wejsciowe to $\\mathrm{e}=e_1...e_m$\n",
    "- każde słowo zagraniczne $f_i$ zostało wygenerowane ze słowa wejściowego $e_{a(j)}$ poprzez funkcję zrównoleglania $a$, z prawdopodobieństwem $t$\n",
    "- $\\epsilon = P(m|e)$ to stała normalizacyjna\n",
    "\n",
    "\n",
    "## Algorytm EM\n",
    "Algorytm EM składa się z dwóch kroków:\n",
    "1. Krok Expectatio: aplikuje model do dnaych\n",
    "    - część modelu jest ukryta (w tym wypadku zrównoleglenie)\n",
    "    - używając modelu przypisujemy prawdopodobieństwo do możliwych wartości\n",
    "2. Krok Maximization: estymuje model na podstawie danych\n",
    "    - przyjmuje przypisane wartości jakofakt\n",
    "    - liczby powiązania (counts) i odpowiadające im wagi (prawdopodobieństwa)\n",
    "    - estymuje nowy model na postawie policzonych powiązań (counts)\n",
    "   \n",
    "Powtarza powyższe kroki aż nie osiągnie zbieżności.\n",
    "\n",
    "### EM - Krok Expectation\n",
    "Musimy oszacować (expect) liczbę razy kiedy słowo $e$ łączy się ze słowem $f$ w tłumaczeniu $(\\mathrm{f}|\\mathrm{e})$. Tą wartość możemy oszacować wzorem:\n",
    "\n",
    "$$ c(f|e;\\mathrm{f};\\mathrm{e})=\\sum_{\\mathrm{a}} P(\\mathrm{a}|\\mathrm{e},\\mathrm{f})\\sum_{j=1}^m \\delta(f,f_j)\\delta(e,e_{a(j)})$$\n",
    "\n",
    "Gdzie ta druga suma oznacza po prostu liczbę łączy słowo $e$ ze słowem $f$ w zdaniu $\\mathrm{a}$.\n",
    "\n",
    "W jaki sposób wyliczyć $p(\\mathrm{a}|\\mathrm{e},\\mathrm{f})$?\n",
    "$$p(\\mathrm{a}|\\mathrm{e},\\mathrm{f}) = \\frac{ p(\\mathrm{f},\\mathrm{a}|\\mathrm{e})}{p(\\mathrm{f}|\\mathrm{e})}$$\n",
    "\n",
    "A w jaki sposób obliczyć samo $p(\\mathrm{f},\\mathrm{a}|\\mathrm{e})$? Jest to wzór zdefiniowany przez Model 1.\n",
    "\n",
    "Jak wyliczyć $p(\\mathrm{f}|\\mathrm{e})$?\n",
    "![alt text](translation1.png \"Sposób obliczenia prawdopodobieństwa tłumaczenia zdania f przez zdanie e\")\n",
    "Sposób obliczenia prawdopodobieństwa tłumaczenia zdania f przez zdanie e\n",
    "\n",
    "\n",
    "### EM - Krok Maximization\n",
    "Pierwszym zadaniem w tym kroku jest policzenie powiązań (counts).\n",
    "Dowód z par zdań $\\mathrm{e}, \\mathrm{f}$, że słowo $f$ jest tłumaczeniem słowa $f$ obliczany jest w następujący sposób:\n",
    "\n",
    "$$c(f|e;\\mathrm{f},\\mathrm{f})=\\sum_a p(\\mathrm{a}|\\mathrm{e}, \\mathrm{f})\\sum_{j=1}^m \\delta(f,f_j)\\delta(e,e_{a(j)})$$\n",
    "\n",
    "A używając uproszenia możemy zapisać to jako:\n",
    "\n",
    "$$c(f|e;\\mathrm{f},\\mathrm{f})=\\frac{t(f|e)}{\\sum_{i=0}^lt(f|e_i)} \\sum_{j=1}^m\\delta(f,f_j)\\delta(e,e_i)$$\n",
    "\n",
    "Po obliczeniu tych wartości możemy oszacować model:\n",
    "\n",
    "$$t(f|e;\\mathrm{e},\\mathrm{f})=\\frac{\\sum_{(\\mathrm{e},\\mathrm{f})}c(f|e;\\mathrm{e},\\mathrm{f})}{\\sum_{f_i}\\sum_{(\\mathrm{e},\\mathrm{f})}c(f_i|e;\\mathrm{e},\\mathrm{f})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flatten = lambda l : [item for sublist in l for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e_s_set = map(str.split, [\"ja mam domek\", \"ja domek mam\", \"domek\", \"ja\"])\n",
    "f_s_set = map(str.split, [\"i have house\", \"i have house\", \"house\", \"i\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def EM(f_s_set, e_s_set):\n",
    "    f_set = set(flatten(f_s_set))\n",
    "    e_set = set(flatten(e_s_set))\n",
    "    \n",
    "    #initialize t(f|e) uniformly\n",
    "    t = {}\n",
    "    for f in f_set:\n",
    "        for e in e_set:\n",
    "            t[(f, e)] = 1.0/len(e_set)\n",
    "\n",
    "    improvement = 1\n",
    "    while improvement > 0.1:\n",
    "        count = {(f, e):0 for f, e in t.keys()}\n",
    "        total = {e:0 for e in e_set}\n",
    "\n",
    "        for f_s, e_s in zip(f_s_set, e_s_set):  # for each pair of sentences\n",
    "            for f in set(f_s):  # for each word\n",
    "                n_f = f_s.count(f)  # count of this word in this sentence\n",
    "                total_s = 0\n",
    "                for e in set(e_s):\n",
    "                    total_s += t[(f, e)] * n_f\n",
    "\n",
    "                for e in set(e_s):\n",
    "                    n_e = e_s.count(e)\n",
    "                    count[(f, e)] += t[(f, e)] * n_f * n_e / total_s\n",
    "                    total[e] += t[(f, e)] * n_f * n_e / total_s\n",
    "\n",
    "                # tu liczymy prawdopodobieństwo warunkowe dla p(f|e)\n",
    "                # total_s to mianowanik - suma wag * liczba słów\n",
    "                # count przechowuje liczniki\n",
    "                # total przechowuje prawdopodobieństwo słowa e\n",
    "\n",
    "        improvement = 0\n",
    "        for f, e in t.keys():\n",
    "            if e in total:\n",
    "                if total[e] > 0:\n",
    "                    new_value = count[(f, e)] / total[e]\n",
    "                    improvement += abs(t[(f, e)] - new_value)\n",
    "                    t[(f, e)] = new_value\n",
    "\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('house', 'domek') 0.971125382219\n",
      "('i', 'ja') 0.971125382219\n",
      "('have', 'mam') 0.846928773971\n",
      "('house', 'mam') 0.0765356130144\n",
      "('i', 'mam') 0.0765356130144\n",
      "('have', 'domek') 0.0264815243342\n",
      "('have', 'ja') 0.0264815243342\n",
      "('i', 'domek') 0.0023930934463\n",
      "('house', 'ja') 0.0023930934463\n"
     ]
    }
   ],
   "source": [
    "t = EM(f_s_set, e_s_set)\n",
    "\n",
    "for pair, score in sorted(t.items(), key=lambda x:x[1], reverse=True):\n",
    "    print pair, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
